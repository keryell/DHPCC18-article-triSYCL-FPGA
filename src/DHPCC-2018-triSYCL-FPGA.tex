%\documentclass[sigplan]{acmart}
\documentclass[sigplan, review, authordraft]{acmart}

\usepackage[latin9]{inputenc}
\usepackage{booktabs} % For formal tables

% To include figures
\usepackage{graphicx}

% For TiKZ plots
\usepackage{tikz,pgfplots}

% To typeset SYCL programs
\usepackage{sycl}

% URL looks nicer in Teletype style
\urlstyle{tt}

% An alternative is
% https://tex.stackexchange.com/questions/266401/frame-a-line-in-an-environnement-lstlisting
% https://tex.stackexchange.com/questions/8851/how-can-i-highlight-some-lines-from-source-code
% /usr/share/doc/texlive-doc/latex/tikzmark/tikzmark.pdf
\usetikzlibrary{tikzmark}
\usetikzmarklibrary{listings}
\usepackage{balloon_rk_no_beamer}


% Copyright
%\setcopyright{none}
%\setcopyright{acmcopyright}
\setcopyright{acmlicensed}
%\setcopyright{rightsretained}
%\setcopyright{usgov}
%\setcopyright{usgovmixed}
%\setcopyright{cagov}
%\setcopyright{cagovmixed}

% pgfplot settings
\pgfplotsset{compat=newest}

\pgfplotsset{
    % #1: index in the group(0,1,2,...)
    % #2: number of plots of that group
    bar group size/.style 2 args={
        /pgf/bar shift={%
                % total width = n*w + (n-1)*skip
                % -> subtract half for centering
                -0.5*(#2*\pgfplotbarwidth +
                (#2-1)* \pgfkeysvalueof{/pgfplots/bar group skip})  +
                % the '0.5*w' is for centering
                (.5+#1)*\pgfplotbarwidth +
                #1*\pgfkeysvalueof{/pgfplots/bar group skip}},%
    },
    bar group skip/.initial=2pt,
    plot 0/.style={black,fill=blue!90!white,mark=none},%
    plot 1/.style={black,fill=blue!30!white,mark=none},%
    plot 2/.style={black,fill=green!90!white,mark=none},%
    plot 3/.style={black,fill=green!30!white,mark=none},%
    plot 4/.style={black,fill=purple!90!white,mark=none},%
    plot 5/.style={black,fill=red!90!white,mark=none},%
}

% DOI
%\acmDOI{10.475/123_4}
%\acmDOI{http://dx.doi.org/10.1145/3078155.3078188}

% ISBN
%\acmISBN{123-4567-24-567/08/06}
%\acmISBN{978-1-4503-5214-7/17/05}

%Conference
%\acmConference[DHPCC'2017]{Distributed \& Heterogeneous Programming in
%  C/C++ Workshop 2017}{May 2017}{Toronto, Canada}
\acmConference{IWOCL '18}{May 14-16, 2018}{Oxford, UK}

\acmMonth{5}
\acmYear{2018}
\copyrightyear{2018}
\acmPrice{15.00}

%\acmBadgeL[http://ctuning.org/ae/ppopp2016.html]{ae-logo}
%\acmBadgeR[http://ctuning.org/ae/ppopp2016.html]{ae-logo}

\sloppy
\begin{document}

\title{Early experiments using SYCL single-source modern C++ on Xilinx FPGA}
\subtitle{Extended Abstract of Technical Presentation}

\author{Ronan Keryell}
\email{rkeryell@xilinx.com}

\author{Lin-Ya Yu}
\email{yu810226@gmail.com}

\affiliation{%
  \institution{Xilinx Research Labs}
  \streetaddress{2020 Bianconi Avenue}
  \city{Dublin}
  \country{Ireland}
  \postcode{D24 T683}
}


\begin{abstract}
  Heterogeneous computing is required in systems ranging from low-end
  embedded systems up to the high-end HPC systems to reach
  high-performance while keeping power consumption low. Having more
  and more CPU and accelerators such as FPGA creates challenges for
  the programmer, requiring even more expertise of them. Fortunately,
  new modern C++-based domain-specific languages, such as the SYCL
  open standard from Khronos Group, simplify the programming at the
  full system level while keeping high performance.

  SYCL is a single-source programming model providing a task graph of
  heterogeneous kernels that can be run on various accelerators or
  even just the CPU. The memory heterogeneity is abstracted through
  buffer objects and the memory usage is abstracted with accessor
  objects. From these accessors, the task graph is implicitly
  constructed, the synchronizations and the data movements across the
  various physical memories are done automatically, by opposition to
  OpenCL or CUDA.

  triSYCL is an on-going open-source project used to experiment with
  the SYCL standard, based on C++17, OpenCL, OpenMP and Clang/LLVM.
  We have extended this framework to target Xilinx SDx tool to compile
  some SYCL programs to run on a CPU host connected to some FPGA PCIe
  cards, by using OpenCL and SPIR standards from Khronos.

  While SYCL provides functional portability, we made a few
  FPGA-friendly extensions to express some optimization to the SDx
  back-end in a pure C++ way.

  We present some interesting preliminary results with simple
  benchmarks showing how to express pipeline, dataflow and
  array-partitioning and we compare with the implementation written
  using other languages available for Xilinx FPGA: HLS C++ and OpenCL
  C.

  %We end up with an experiment using the current CNL proposal for
  %fixed-point data types in ISO C++, giving guidance to the ISO
  %committee and demonstrating the advantage of pure standard C++ for
  %all-programmable devices beyond CPU, such as FPGA MPSoC.
\end{abstract}

%
% The code below should be generated by the tool at
% http://dl.acm.org/ccs.cfm
\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10010147.10010169.10010175</concept_id>
<concept_desc>Computing methodologies~Parallel programming languages</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10010147.10010169.10010170.10010174</concept_id>
<concept_desc>Computing methodologies~Massively parallel algorithms</concept_desc>
<concept_significance>300</concept_significance>
</concept>
<concept>
<concept_id>10011007.10011006.10011008.10011009.10010175</concept_id>
<concept_desc>Software and its engineering~Parallel programming languages</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10011007.10011006.10011050.10011017</concept_id>
<concept_desc>Software and its engineering~Domain specific languages</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10011007.10011006.10011066.10011067</concept_id>
<concept_desc>Software and its engineering~Object oriented frameworks</concept_desc>
<concept_significance>300</concept_significance>
</concept>
<concept>
<concept_id>10010520.10010521.10010542.10010546</concept_id>
<concept_desc>Computer systems organization~Heterogeneous (hybrid) systems</concept_desc>
<concept_significance>300</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computing methodologies~Parallel programming languages}
\ccsdesc[300]{Computing methodologies~Massively parallel algorithms}
\ccsdesc[500]{Software and its engineering~Parallel programming languages}
\ccsdesc[500]{Software and its engineering~Domain specific languages}
\ccsdesc[300]{Software and its engineering~Object oriented frameworks}
\ccsdesc[300]{Computer systems organization~Heterogeneous (hybrid) systems}
% We no longer use \terms command
%\terms{Theory}

\keywords{SYCL, C++17, DSeL, FPGA, reconfigurable computing, OpenCL,
  SPIR, Clang, LLVM, triSYCL}


\maketitle

\section{Introduction}
\label{sec:introduction}

Computing architectures nowadays are huge hybrid multi-processor
system-on-chips with different kinds of processors, GPU, configurable
specific accelerators (video CODEC...), reconfigurable programmable
logic (FPGA), various hierarchies of memory and memory interfaces,
configurable IO and network support, security support, and power control
etc. High-performance applications may use a hierarchy of such systems
scaling up towards utilizing a full-scale data-center.

So the programmer is facing a fractal architecture, demanding
also more and more control for power efficiency. This tends to require
a dense fractal set of skills and tools.

SYCL \cite{SYCL-1.2.1,SYCL-2.2-provisional} is a new open standard
from Khronos Group aiming at solving some of the programming issues
related to heterogeneous computing.  This pure C++14 (for SYCL 1.2.1)
or C++17 (for SYCL 2.2) domain-specific embedded language allows the
programmer to write single-source C++ host code with accelerated code
expressed as functors. The data accesses are described with accessor
objects that implicitly define a task graph that can be asynchronously
scheduled on a distributed-memory system including several CPU and
accelerators.

This programming model is quite generic but provides also an
interoperability mode with the OpenCL realm, another standard from
Khronos Group aimed at heterogeneous computing with a C host API and
separate language for the kernels (C, C++, SPIR and SPIR-V).  This
allows a SYCL C++ application to recycle existing OpenCL kernels into
a higher level C++ programming model, relieving the programmer from
explicitly defining the memory transfers.

In this article we present in Section~\ref{sec:sycl} the SYCL
standard, then in Section~\ref{sec:sycl-opencl-inter} SYCL's interoperability mode with OpenCL, in
Section~\ref{sec:exper-with-opencl} some experiments with the
triSYCL open source implementation of the SYCL standard and comparing
to some related work in Section~\ref{sec:related-work}.


\section{SYCL}
\label{sec:sycl}

\begin{figure*}
  \balloono{comment}{example}{17}{26}
  \balloon{comment}{example}{22}{25}
  \balloono{comment}{example}{28}{37}
  \balloon{comment}{example}{33}{36}
  \balloono{comment}{example}{39}{50}
  \balloon{comment}{example}{46}{49}
  \begin{tikzpicture}[remember picture,overlay]
    \draw [overlay,->,very thick,red] (pic cs:PA) to[bend left] (pic cs:CA);
    \draw [overlay,->,very thick,red] (pic cs:PB) to[bend left] (pic cs:CB);
    \draw [overlay,->,very thick,red] (pic cs:PC) to[bend right] (pic cs:CC);
  \end{tikzpicture}
  \begin{tabular}{c}
    \begin{lstlisting}[basicstyle=\scriptsize,name=example]
// Demonstrate the use of an asynchronous task graph of kernels to initialize and add 2 matrices.
#include <CL/sycl.hpp>
#include <iostream>
using namespace cl::sycl;
// Size of the matrices
constexpr size_t N = 2000;
constexpr size_t M = 3000;

int main() {
  // Create a queue to work on
  queue q;
  // Create some 2D buffers of N*M floats for our matrices
  buffer<float, 2> a { { N, M } };
  buffer<float, 2> b { { N, M } };
  buffer<float, 2> c { { N, M } };
  // Launch a first asynchronous kernel to initialize a
  q.submit([&] (handler &cgh) {
      // The kernel writes a, so get a write accessor on it
      auto A = a.get_access<access::mode::write>(cgh);¾\tikzmark{PA}¾

      // Enqueue a parallel kernel iterating on a N*M 2D iteration space
      cgh.parallel_for<class init_a>({ N, M },
                                     [=] (id<2> index) {
                                       A[index] = index[0]*2 + index[1];
                                     });
    });
  // Launch an asynchronous kernel to initialize b
  q.submit([&] (handler &cgh) {
      // The kernel writes b, so get a write accessor on it
      auto B = b.get_access<access::mode::write>(cgh);¾\tikzmark{PB}¾
      /* From the access pattern above, the SYCL runtime detects this command group is independent from the first one and can be scheduled independently.
         Enqueue a parallel kernel iterating on a N*M 2D iteration space */
      cgh.parallel_for<class init_b>({ N, M },
                                     [=] (id<2> index) {
                                       B[index] = index[0]*2014 + index[1]*42;
                                     });
    });
  // Launch an asynchronous kernel to compute matrix addition c = a + b
    q.submit([&] (handler &cgh) {
        // In the kernel a and b are read, but c is written
        auto A = a.get_access<access::mode::read>(cgh);¾\tikzmark{CA}¾
        auto B = b.get_access<access::mode::read>(cgh);¾\tikzmark{CB}¾
        auto C = ¾\tikzmark{PC}¾c.get_access<access::mode::write>(cgh);
        // From these accessors, the SYCL runtime will ensure that when this kernel is run, the kernels computing
        // a and b completed. Then Enqueue a parallel kernel iterating on a N*M 2D iteration space
        cgh.parallel_for<class matrix_add>({ N, M },
                                           [=] (id<2> index) {
                                             C[index] = A[index] + B[index];
                                           });
      });
    /* Request an accessor to read c from the host-side. The SYCL runtime ensures that c is ready when the accessor is returned */
    auto C = ¾\tikzmark{CC}¾c.get_access<access::mode::read>();
    std::cout << std::endl << "Result:" << std::endl;
    for (size_t i = 0; i < N; i++)
      for (size_t j = 0; j < M; j++)
        // Compare the result to the analytic value
        if (C[i][j] != i*(2 + 2014) + j*(1 + 42)) {
          std::cout << "Wrong value " << C[i][j] << " on element "
                    << i << ' ' << j << std::endl;
          exit(-1);
        }
  std::cout << "Accurate computation!" << std::endl;
  return 0;
}
    \end{lstlisting}
  \end{tabular}
  \caption{Example of a SYCL C++ program producing and adding 2
    matrices, coming from
    \url{https://github.com/triSYCL/triSYCL/blob/master/tests/examples/demo_parallel_matrix_add.cpp}.\label{fig:SYCL-example}}
\end{figure*}

SYCL \cite{SYCL-1.2.1,SYCL-2.2-provisional} (pronounced ``sickle'') is a
royalty-free, cross-platform abstraction C++ programming model for
OpenCL \cite{OpenCL-API-2.2-provisional,
  OpenCL-C++-2.2-provisional}. SYCL builds on the underlying concepts,
portability and efficiency of OpenCL while adding much of the ease of
use and flexibility of single-source C++.

Developers using SYCL are able to write standard C++14/C++17 code,
with many of the techniques they are accustomed to, such as
inheritance and templating. At the same time developers have access to
the full range of capabilities of OpenCL both through the features of
the SYCL libraries and, where necessary, through interoperation with
code written directly to the OpenCL
APIs~\cite{OpenCL-API-2.2-provisional}.

SYCL implements a single-source multiple compiler-passes design which
offers the power of source integration while allowing tool-chains to
remain flexible. This design supports embedding of code intended to be
compiled for an OpenCL device, for example a GPU or an FPGA, inline
with host code. This embedding of code offers three primary benefits:
\begin{description}
\item[simplicity:] for novice programmers, the separation of host and
  device source code in OpenCL can become complicated to deal with,
  particularly when similar kernel code is used for multiple different
  operations. A single compiler flow and integrated tool chain
  combined with libraries that perform a lot of simple tasks
  simplifies initial OpenCL programs to a minimum complexity. This
  reduces the learning curve for programmers new to OpenCL and allows
  them to concentrate on parallelization techniques rather than
  syntax;
\item[reuse:] C++'s type system allows for complex interactions
  between different code units and supports efficient abstract
  interface design and reuse of library code.  For example, a C++
  \lstinline{std::transform} or \lstinline{std::for_each} algorithm
  applied to an array of data may allow specialization on both the
  operation applied to each element of the array and on the type of
  the data;
\item[efficiency:] tight integration with the type system and reuse of
  library code enables a compiler to perform inlining of code and to
  produce efficient specialized device code based on decisions made in
  the host code without having to generate kernel source strings
  dynamically as done with other frameworks \cite{VexCL,
    Boost.Compute}.
\end{description}

SYCL is a pure single-source C++ DSeL (Domain-Specific Embedded
Language) providing simpler abstractions for heterogeneous
computing. In Figure~\ref{fig:SYCL-example} is presented a small
application using SYCL concepts to create a graph of 3 asynchronous
tasks to initialize 2 matrices and addition them before checking for
the final result.

The main interesting features that SYCL brings are:
\begin{description}

\item[asynchronous task graphs] to break an application in parallel
  pieces able to run on various accelerators or on the host, taking
  advantage of the CPU cores and accelerators of the platform;

\item[hierarchical parallelism] to take advantage inside a task of the
  common intrinsic parallelism found in accelerators as shown on
  Figure~\ref{fig:OpenCL-execution-model}, that can be expressed
  either as in OpenCL with ND-ranges (multi-dimensional iteration
  spaces) or in a similar hierarchical way to how it is done in OpenMP
  \cite{OpenMP-5.0-preview-1}, but with a nicer C++-friendly method
  based on lambda functions. The simpler hierarchical way relieves the
  programmer from using painful work-group synchronizations and is
  also more efficient on CPU and FPGA;

  \begin{figure}
    %\includegraphics[width=\hsize]{figures/execution-model-no_GPU}
    \caption{OpenCL execution model, with the parallel iteration space
      of work-groups of work-items mapped for execution onto independent
    compute units composed of processing elements.}
    \label{fig:OpenCL-execution-model}
  \end{figure}

\item[buffers] defining location-independent storage, (no explicit
  move) usable as multi-dimensional arrays to be used from the various
  CPU cores and accelerators;

\item[accessors] to express usage for buffers and other objects with
  some attributes such as read/write/\ldots, the location or the kind
  of memory to use, allowing finer control on the complex memory
  hierarchy found on accelerators as shown on
  Figure~\ref{fig:OpenCL-memory-model} to reach the maximum power and
  compute efficiency;

  \begin{figure}
    %\includegraphics[width=\hsize]{figures/memory-model}
    \caption{OpenCL memory model, with the 4 different explicit kinds
      of memory, besides the host memory.}
    \label{fig:OpenCL-memory-model}
  \end{figure}

\item[implicit dependency graph] construction is done with the
  separation in SYCL of the data access from data storage. By relying
  on the C++-style RAII (resource acquisition is initialization) idiom
  on accessors, the runtime library can capture data dependencies
  between device code blocks and construct the task graph implicitly
  with all the dependencies;

\item[automatic data motion] is then done by the runtime by tracking
  the data through the accessors ahead of time, making sure the data
  are available when needed by a kernel on a device or by the host,
  without the programmer requiring like in OpenCL or in CUDA to
  explicitly move the data;

\item[overlapping kernels and communications] is provided
  automatically by the SYCL scheduler by using the dependency graph
  between tasks without requiring the programmer to manage explicitly
  several command queues and synchronizing events for this;

\item[single-source] programming model, similar to OpenMP
  simplicity and type safety but in a pure C++-friendly world, without
  requiring \lstinline{#pragma} that do not compose nicely with
  C++. This allows the writing of high-level programming and
  meta-programming. For example on the Figure~\ref{fig:SYCL-example},
  except in the buffer creations on lines 17--19, the data type does
  not appear anywhere in the code and is just inferred by the compiler
  even across the host-device boundary;

\item[host fallback] is a by-product of having a pure C++ DSeL. By
  just providing a C++ implementation of the SYCL runtime on the
  host. Thus the same code can work either on the host CPU or on the
  device, allowing more parallelism but also just to run even if a
  device is missing;

\item[host debugging] for free is a nice side-effect of the host
  implementation of SYCL. The development of heterogeneous
  applications is quite challenging but having the same code running
  on the host allows the use of the normal C++ development tools
  chains, from high-end static analysis tools, dynamic thread or
  memory checkers, debuggers, watch-points, etc.  down to inserting
  plain standard I/O messages in the code;

\item[host emulation] is also for free, which interesting in the case
  of the FPGA world where synthesizing the code for the device
  is an offline activity compared to running the code on CPU or even on GPU;

\item[cross-platform] support allows to have buffers used by different
  devices from different vendors in a seamless way, which is not
  possible directly in OpenCL.

  % Skip on purpose the OpenCL interoperability mode here

\end{description}

SYCL retains the execution model, runtime feature set and device
capabilities of the underlying OpenCL standard. This is why SYCL
1.2.1~\cite{SYCL-1.2.1} targets devices with OpenCL
1.2~\cite{OpenCL-API+C-1.2} capabilities, while SYCL
2.2~\cite{SYCL-2.2-provisional} targets devices with OpenCL
2.2~\cite{OpenCL-API-2.2-provisional} capabilities, adding for example
the pipes and the shared virtual memory between the host and devices.

The OpenCL C specification imposes some limitations on the full range
of C++ features that SYCL is able to support. This ensures portability
of device code across as wide a range of devices as possible.

As a result, while the code can be written in standard C++ syntax with
interoperability with standard C++ programs, the entire set of C++
features is not available in SYCL device code. In particular, SYCL
device code, as defined by this specification, does not support
virtual function calls, function pointers in general, exceptions,
runtime type information or the full set of C++ libraries that may
depend on these features or on features of a particular host compiler.

These features are not often used in high-performance code even
in plain C++ in the hot-path because of performance issues.
Fortunately, the use of C++ features such as templates and inheritance
on top of the OpenCL execution model opens a wide scope for innovation
in software design for heterogeneous systems, giving workarounds for
some of the unsupported features.

Clean integration of device and host code within a single C++ type
system enables the development of modern, templated libraries that
build simple, yet efficient, interfaces to offer more developers
access to OpenCL capabilities and devices. SYCL is intended to serve
as a foundation for innovation in programming models for heterogeneous
systems, that builds on an open and widely implemented standard
foundation in the form of OpenCL.

This is why the OpenCL version of TensorFlow \cite{TensorFlow-1.0},
the C++ machine learning framework from Google, is actually using SYCL
instead of plain OpenCL.

SYCL is one of the candidates giving inputs on parallelism and
heterogeneous computing to the C++ ISO/IEC JTC1/SC22/WG21
standardization committee
\cite{C++:P00236R0:SYCL,C++:P0362R0,C++:P0363R0,C++:P0367R0}.
\iffalse
  WG21,
presented at C++ F2F committee Jacksonville 2016/02
% http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2016/p0236r0.pdf
% https://groups.google.com/a/isocpp.org/forum/#!topic/sg14/8GWWDulGE7o
 Oulu 2016/06
% http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2016/p0362r0.pdf
% http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2016/p0363r0.pdf
% http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2016/p0367r0.pdf
 Khronos committed to provide Open Source implementations of OpenCL and SYCL
\fi


\section{Fine grain parallel execution}
\label{sec:fine-grain-parallel}

High-level programs on FPGA are typically compiled with each operation
translated in some hardware implementation. Compare to the compilation for
architectures with fixed hardware functions such as CPU, DSP or GPU, that
means that the hardware implementations of the operations do coexist in
space inside the FPGA, which means that they can be used in parallel to
increase the throughput of the system.

In a ``normal'' computer this is not a parameter to be taken in
consideration, but for FPGA this is something useful to express somehow,
when compiling a C/C++ program for FPGA. Each FPGA vendor have some
extensions to express these optimizations, such as
\cite{IWOCL2016:OpenCL_Xilinx_FPGA,
  Xilinx:UG1207-2017.4:SDAccel_Profiling_Optimization,
  Xilinx:UG1235-2017.4:SDSoC_Profiling_Optimization,
  Xilinx:UG1270-2017.4:HLS_Optimization_Methodology,
  Xilinx:UG1253-2017.4:SDx_Pragma_Reference, altera}, based on
\lstinline|#pragma|, attributes or even configuration files or compiler
options.

The issues  with extensions such  as \lstinline|#pragma| or  attributes is
that they require a specific compiler  to parse and understand them.  This
prevents some  emulation on  CPU for example,  with some  compile-time and
run-time checking.

In our SYCL implementation, we prefer to have some pure C++ function-based
and class-based extensions, compatible with CPU emulation and more aligned
with SYCL DSeL philosophy.


\subsection{Pipelined execution}
\label{sec:pipelined-execution}

\begin{figure}
  \includegraphics[width=\hsize]{Figures/pipeline_execution}
  \caption{Execution of operations of a loop in normal and pipelined way.}
  \label{fig:pipelined-execution}
\end{figure}

In a loop, all the operations of an iteration will be executed
sequentially, as shown on Figure~\ref{fig:pipelined-execution}. The
operations are used in sequence and if there is no intra-loop dependency
in the program preventing this, the operations from different iterations
can actually be executed in parallel, to increase the throughput using the
pipelined execution shown also on Figure~\ref{fig:pipelined-execution}, by
reducing the \emph{initiation interval}, to use the FPGA jargon.

\begin{figure}
  \balloon{comment}{PipelinedTriSYCL}{5}{5}
  \balloon{comment}{PipelinedTriSYCL}{9}{9}
  \begin{tabular}{c}
    \begin{lstlisting}[basicstyle=\scriptsize,name=PipelinedTriSYCL]
template<typename T, typename U>
void compute(T *buffer_in, U *buffer_out) {
  for(int i = 0; i < NUM_ROWS; ++i) {
    for (int j = 0; j < ELE_PER_ROW; ++j) {
      xilinx::pipeline([&] {
        int inTmp = buffer_in[ELE_PER_ROW*i+j];
        int outTmp = inTmp * ALPHA;
        buffer_out[ELE_PER_ROW*i+j] = outTmp;
      });
    }
  }
}
    \end{lstlisting}
  \end{tabular}
  \caption{Example of a typical loop with pipelined execution. What is
    added is shown with a blue background.}
  \label{fig:SYCL-pipelined}
\end{figure}

As shown on Figure~\ref{fig:SYCL-pipelined} we chose to mark a zone to be
pipelined with a kind of decorator \emph{à la} Python
\cite{Python:PEP318:decorators} \lstinline|xilinx::pipeline(...)| taking a
functor, typically expressed in modern C++ as a lambda expression.


\subsection{Dataflow execution}
\label{sec:dataflow-execution}

While pipeline execution is very fine-grain parallelism, a coarser-grain
parallelism can be used when having different functions with data flowing
from one function to another: dataflow execution.

\begin{figure}
  \includegraphics[width=\hsize]{Figures/dataflow_execution}
  \caption{Execution of functions without or with dataflow mode.}
  \label{fig:dataflow-execution}
\end{figure}

Since on the FPGA the functions are instantiated in hardware, they can
execute in parallel and if data dependencies allow it, data can move from
one function to another either using dual-buffering ping-pong buffers
(preventing the time-consuming and power-hungry external memory) or even a
FIFO (preventing the use of more complex power-hungry memory access), as
shown on \ref{fig:dataflow-execution}.

\begin{figure}
  \balloon{comment}{DataFlowTriSYCL}{5}{5}
  \balloon{comment}{DataFlowTriSYCL}{7}{7}
  \balloon{comment}{DataFlowTriSYCL}{14}{14}
  \balloon{comment}{DataFlowTriSYCL}{18}{18}
  \balloon{comment}{DataFlowTriSYCL}{25}{25}
  \balloon{comment}{DataFlowTriSYCL}{27}{27}
  \balloon{comment}{DataFlowTriSYCL}{27}{27}
  \balloono{comment}{DataFlowTriSYCL}{37}{37}
  \balloono{comment}{DataFlowTriSYCL}{41}{41}
  \begin{tabular}{c}
    \begin{lstlisting}[basicstyle=\scriptsize,name=DataFlowTriSYCL]
template<typename T, typename U>
void readInput(T *buffer_in, const U &d_b) {
  for(int i = 0; i < NUM_ROWS; ++i)
    for (int j = 0; j < ELE_PER_ROW; ++j)
      xilinx::pipeline([&] {
        buffer_in[ELE_PER_ROW*i+j] = d_b[ELE_PER_ROW*i+j];
      });
}

template<typename T, typename U>
void compute(T *buffer_in, U *buffer_out) {
  for(int i = 0; i < NUM_ROWS; ++i)
    for (int j = 0; j < ELE_PER_ROW; ++j)
      xilinx::pipeline([&] {
        int inTmp = buffer_in[ELE_PER_ROW*i+j];
        int outTmp = inTmp * ALPHA;
        buffer_out[ELE_PER_ROW*i+j] = outTmp;
      });
}

template<typename T, typename U>
void writeOutput(T *buffer_out, const U &d_a) {
  for(int i = 0; i < NUM_ROWS; ++i)
    for (int j = 0; j < ELE_PER_ROW; ++j)
      xilinx::pipeline([&] {
        d_a[ELE_PER_ROW*i+j] = buffer_out[ELE_PER_ROW*i+j];
      });
}

cgh.single_task<class add>
           ([=,
             d_a = drt::accessor<decltype(a_a)> { a_a },
             d_b = drt::accessor<decltype(a_b)> { a_b }
             ] {
                 int buffer_in[BLOCK_SIZE];
                 int buffer_out[BLOCK_SIZE];
                 xilinx::dataflow([&] {
                     readInput(buffer_in, d_b);
                     compute(buffer_in, buffer_out);
                     writeOutput(buffer_out, d_a);
                 });
               });
    \end{lstlisting}
  \end{tabular}
  \caption{Example of a typical loop with functional dataflow execution
    with also pipelined execution of each function. What is added for
    dataflow execution has an orange background and for a pipelined
    execution has a blue background.}
  \label{fig:SYCL-dataflow}
\end{figure}

To mark an area in the program to apply such an optimization, we use the
same trick as for pipelining (\S~\ref{sec:pipelined-execution}) with a
\lstinline|xilinx::dataflow(...)| as shown on
Figure~\ref{fig:SYCL-dataflow}. Currently we have a limitation in our
device compiler and runtime that requires changing the lambda captures
with \lstinline|drt::accessor| like adding in the example some lines such
as
\begin{lstlisting}[basicstyle=\scriptsize,name=DataFlowTriSYCL]
d_a = drt::accessor<decltype(a_a)> { a_a },
\end{lstlisting}
but this limitation should be removed soon.


\section{Array partitioning}
\label{sec:array-partitioning}

On FPGA even the memories are configurable and can be implemented with
different level of parallelism, providing some trade-off between hardware
complexity (cost), bandwidth and latency.

\begin{figure}
  \includegraphics[width=\hsize]{Figures/cyclic_partition_array}
  \caption{Array of 16 elements distributed in a cyclic way on 4 memory
    banks. The numbers represent the indices of the array elements stored
    on the memory banks.}
  \label{fig:cyclic-array}
\end{figure}

\begin{figure}
  \includegraphics[width=\hsize]{Figures/block_partition_array}
  \caption{Array of 16 elements distributed by blocks on 4 memory
    banks. The numbers represent the indices of the array elements stored
    on the memory banks.}
  \label{fig:block-array}
\end{figure}

\begin{figure}
  \includegraphics[width=\hsize]{Figures/complete_partition_array}
  \caption{Complete partition of an array of 16 elements, with 1 element
    by register.}
  \label{fig:complete-partition-array}
\end{figure}

For example an array in a program on an FPGA can be distributed on
different memory banks in a cyclic way (Figure~\ref{fig:cyclic-array}),
partitioned by block (Figure~\ref{fig:block-array}) or fully partitioned
(Figure~\ref{fig:complete-partition-array}) for a maximum bandwidth but at
the cost of a very complex routing.

Since in C++ there is a \lstinline|std::array| we introduced a new
\lstinline|cl::sycl::xilinx::partition_array| similar to
\lstinline|std::array| but taking a new templated class parameter to
model the partition.

For now we support only 1-dimension arrays similar to
\lstinline|std::array| because we are waiting for the ISO C++
standardization committee to define multidimensional arrays
\cite{C++:P0xyzrv:md-span} so we can align on the same syntax.

\begin{figure*}
  \balloono{comment}{PartitionCyclicblockTriSYCL}{8}{9}
  \balloono{comment}{PartitionCyclicblockTriSYCL}{12}{13}
  \balloono{comment}{PartitionCyclicblockTriSYCL}{14}{15}
  \balloon{comment}{PartitionCyclicblockTriSYCL}{23}{23}
  \balloon{comment}{PartitionCyclicblockTriSYCL}{26}{26}
  \balloon{comment}{PartitionCyclicblockTriSYCL}{31}{31}
  \balloon{comment}{PartitionCyclicblockTriSYCL}{34}{34}
  \balloon{comment}{PartitionCyclicblockTriSYCL}{42}{42}
  \balloon{comment}{PartitionCyclicblockTriSYCL}{48}{48}
  \balloon{comment}{PartitionCyclicblockTriSYCL}{55}{55}
  \balloon{comment}{PartitionCyclicblockTriSYCL}{58}{58}
  \begin{tabular}{c}
    \begin{lstlisting}[basicstyle=\scriptsize,name=PartitionCyclicblockTriSYCL]
cgh.single_task<class add>([=,
                            d_out = drt::accessor<decltype(a_out)> { a_out },
                            d_in1 = drt::accessor<decltype(a_in1)> { a_in1 },
                            d_in2 = drt::accessor<decltype(a_in2)> { a_in2 }
] {
      // Cyclic Partition for A as matrix multiplication needs row-wise
      // parallel access
      xilinx::partition_array<Type, BLOCK_SIZE,
                              xilinx::partition::cyclic<MAX_DIM>> A;
      // Block Partition for B as matrix multiplication needs column-wise
      // parallel access
      xilinx::partition_array<Type, BLOCK_SIZE,
                              xilinx::partition::block<MAX_DIM>> B;
      xilinx::partition_array<Type, BLOCK_SIZE> C;

      // As A and B Matrix are partitioned with the factor of MAX_DIM, so
      // to get parallel row/column access, input square matrix[DIMXDIM]
      // should be written into local Array in MATRIX[MAX_DIM * MAX_DIM]
      // format

      // Burst read for matrix A
      for (int itr = 0, i = 0, j = 0; itr < DIM * DIM; itr++, j++) {
        xilinx::pipeline([&] {
          if (j == DIM) { j = 0; i++; }
          A[i*MAX_DIM + j] = d_in1[itr];
        });
      }

      // Burst read for matrix B
      for (int itr = 0, i = 0, j = 0; itr < DIM * DIM; itr++, j++) {
        xilinx::pipeline([&] {
          if (j == DIM) { j = 0; i++; }
          B[i * MAX_DIM + j] = d_in2[itr];
        });
      }

      for (int i = 0; i < DIM; i++) {
        // As A and B are partition correctly so loop pipelining is
        // applied at 2nd level loop and which will eventually unroll
        // the lower loop
        for (int j = 0; j < DIM ; j++) {
          xilinx::pipeline([&] {
            int result = 0;
            for (int k = 0; k < MAX_DIM; k++) {
                result += A[i * MAX_DIM +  k] * B[k * MAX_DIM + j];
            }
            C[i*MAX_DIM + j] = result;
          });
        }
      }

      // Burst write from output matrices to global memory
      // Burst write from matrix C
      for (int itr = 0, i = 0, j = 0; itr < DIM * DIM; itr++, j++) {
        xilinx::pipeline([&] {
          if (j == DIM) { j = 0; i++; }
          d_out[itr] = C[i * MAX_DIM + j];
        });
      }

  });
    \end{lstlisting}
  \end{tabular}
  \caption{Example of a matrix multiplication using partitioned
    arrays (orange background) and pipelining (blue background).}
  \label{fig:SYCL-partition-cyclic-block}
\end{figure*}

The example on Figure~\ref{fig:SYCL-partition-cyclic-block} show how
partitioned arrays are used to speed-up the computation, in
combination with pipelining, by reducing the bank conflicts even if
the pipelining increase the bandwidth of the computation.

\begin{figure}
  \begin{tabular}{c}
    \begin{lstlisting}[basicstyle=\scriptsize,name=PartitionCyclicblockTriSYCL]
xilinx::partition_array<float, 16,
                        xilinx::partition::complete<>> h;
     \end{lstlisting}
  \end{tabular}
  \caption{Complete partition of an array of 16 elements, with 1 element
    by register.}
  \label{fig:SYCL-complete-partition-array}
\end{figure}

In a similar way a complete partitioning could be used as shown on
Figure~\ref{fig:SYCL-complete-partition-array}.


\section{SYCL and OpenCL interoperability mode}
\label{sec:sycl-opencl-inter}

SYCL is a very generic data-parallel task graph model implemented as a
C++ DSeL that often relies on OpenCL and SPIR behind the hood to
target accelerators, but it could use some other technology.

There is also in SYCL a specific OpenCL interoperability mode if
needed, allowing direct interaction with the OpenCL world, and by
transitivity to Vulkan/OpenGL/DirectX/... In this way it is possible
to use existing programs or libraries with no overhead.

There are 2 main parts in the interoperability mode:
\begin{enumerate}
\item it is possible to construct SYCL objects from existing OpenCL
  objects to run SYCL single-source programs in relation with an
  existing OpenCL framework. For example a SYCL buffer can be
  constructed from an OpenCL \lstinline|cl_mem| or SYCL queue from a
  \lstinline|cl_command_queue|;
\item it is possible to get some OpenCL objects from higher-level SYCL
  objects to execute plain OpenCL code from the SYCL world, for
  example launching an OpenCL kernel that uses an implicit
  \lstinline|cl_mem| associated to a SYCL accessor.
\end{enumerate}

Whereas the interoperability mode was included in the SYCL standard to
extend the applicability of SYCL on the OpenCL realm, it appears that
this mode is useful by itself to do plain OpenCL programming in higher
level C++, in the same way there already exists various C++ OpenCL
wrapper.

While it does not take advantage from the single-source programming
style of SYCL, it has some value for OpenCL programmers as it
simplifies the boilerplate and housekeeping. For example, it allows to
use the task graph model on top of OpenCL kernels and the synergy
between buffers and accessors relieves the programmer from managing
explicitly the buffer content transfers between host and devices.

In the following we present some use case of OpenCL interoperability
using the triSYCL implementation.


\section{Experimenting OpenCL interoperability mode of SYCL with
  triSYCL}
\label{sec:exper-with-opencl}

triSYCL \cite{triSYCL} is an open-source implementation based on C++17
and various Boost libraries \cite{Boost-1.63}. The host acceleration
is provided with OpenMP. The device compiler used to outline the
kernels in single-source mode is based on Clang \cite{Clang-4.0}/LLVM
\cite{LLVM-4.0}. The OpenCL interoperability mode is based on
Boost.Compute \cite{Boost.Compute}.

To even simplify further the programming style, triSYCL
implements Boost.Compute interoperability mode too, as an extension to
the SYCL standard.


\begin{figure*}
  \begin{tabular}{c}
    \begin{lstlisting}[basicstyle=\scriptsize]
  [...]

int search_image(buffer<int>& training, const Img& img, queue& q, const kernel& k) {
  int res[training_set_size];

  {
    buffer<int> A { std::begin(img.pixels), std::end(img.pixels) };
    buffer<int> B { res, training_set_size };
    // Compute the L2 distance between an image and each one from the
    // training set
    q.submit([&] (handler &cgh) {
        // Set the kernel arguments. The accessors lazily trigger data
        // transfers between host and device only if necessary. For
        // example "training" is only transfered the first time the
        // kernel is executed.
        cgh.set_args(training.get_access<access::mode::read>(cgh),
                     A.get_access<access::mode::read>(cgh),
                     B.get_access<access::mode::discard_write>(cgh),
                     int { training_set_size }, int { pixel_number });
        // Launch the kernel with training_set_size work-items
        cgh.parallel_for(training_set_size, k);
      });
    // The destruction of B here waits for kernel execution and copy
    // back the data to res
  }

  // Find the image with the minimum distance
  auto min_image = std::min_element(std::begin(res), std::end(res));

  // Test if we found the correct digit
  return
    training_set[std::distance(std::begin(res), min_image)].label == img.label;
}

int main(int argc, char* argv[]) {
  int correct = 0;
  training_set = slurp_file("data/trainingsample.csv");
  validation_set =  slurp_file("data/validationsample.csv");
  buffer<int> training_buffer = get_buffer(training_set);

  // A SYCL queue to send the heterogeneous work-load to
  queue q { boost::compute::system::default_queue() };

  // Use real OpenCL program for the kernel
  auto program = boost::compute::program::create_with_source(R"(
    __kernel void kernel_compute(__global const int* trainingSet,
                                 __global const int* data,
                                 __global int* res, int setSize, int dataSize) {
      int diff, toAdd, computeId;
      computeId = get_global_id(0);
      if (computeId < setSize) {
        diff = 0;
        for (int i = 0; i < dataSize; i++) {
            toAdd = data[i] - trainingSet[computeId*dataSize + i];
            diff += toAdd * toAdd;
        }
        res[computeId] = diff;
      }
    }
    )", boost::compute::system::default_context());

  program.build();

  // Construct a SYCL kernel from OpenCL kernel to be used in
  // interoperability mode
  kernel k { boost::compute::kernel { program, "kernel_compute"} };

  // Match each image from the validation set against the images from
  // the training set
  for (auto const & img : validation_set)
    correct += search_image(training_buffer, img, q, k);

  [...]

  return 0;
}
    \end{lstlisting}
  \end{tabular}
  \caption{Digit recognition application using SYCL with OpenCL
    interoperability mode.\label{fig:digit-SYCL-OpenCL-code}}
\end{figure*}

We have used triSYCL to simplify the OpenCL host-side programming of
some machine learning examples.


\subsection{Handwritten digit recognition in gray images using L2
  norm}
\label{sec:example-from-anast}

To benchmark various languages and frameworks, we used an application
for recognition of handwritten digits in 8-bit gray images of
$28 \times 28$ pixels.

There are 500 images compared against a training reference
set of labeled digit images. A kernel is launched to compare each
image against all the reference images.

\begin{figure}
  \begin{tikzpicture}
    \begin{axis}[
        ybar,
        axis on top,
        ymax=1.2, ymin=0,
        height=6cm,
        width=\hsize,
        enlarge x limits=0.12,
        ymajorgrids,
        tick align=outside,
        major grid style={draw=none},
        axis x line*=left,
        ylabel={ms},
        xtick={triSYCL OMP,triSYCL OCL,ComputeCPP,OpenCL},
        max space between ticks=30pt,
        x tick label style={/pgf/number format/1000 sep=},
        legend style={at={(0.5,-0.15)},
        anchor=north,legend columns=-1},
        bar width=6pt,
        symbolic x coords={triSYCL OMP,triSYCL OCL,ComputeCPP,OpenCL},
        nodes near coords=\rotatebox{90}{\scriptsize\pgfmathprintnumber
          \pgfplotspointmeta}
        ]
        \addplot[plot 0,bar group size={0}{4}]
        coordinates {(triSYCL OMP,0.99)};
        \addplot[plot 0,bar group size={1}{4}]
        coordinates {(triSYCL OMP,0.75)};
        \addplot[plot 1,bar group size={2}{4}]
        coordinates {(triSYCL OMP,0.75)};
        \addplot[plot 1,bar group size={3}{4}]
        coordinates {(triSYCL OMP,0.43)};

        \addplot[plot 2,bar group size={0}{4}]
        coordinates {(triSYCL OCL,0.48)};
        \addplot[plot 2,bar group size={1}{4}]
        coordinates {(triSYCL OCL,0.44)};
        \addplot[plot 3,bar group size={2}{4}]
        coordinates {(triSYCL OCL,0.47)};
        \addplot[plot 3,bar group size={3}{4}]
        coordinates {(triSYCL OCL,0.44)};

        \addplot[plot 4,bar group size={0}{1}]
        coordinates {(ComputeCPP,0.77)};

        \addplot[plot 5,bar group size={0}{1}]
        coordinates {(OpenCL,0.42)};

    \end{axis}
  \end{tikzpicture}
  \caption{Average total execution time for digit recognition
    application per image running OpenCL and SYCL kernels with different host
    API and configurations on a CPU.}
  \label{fig:results-digit-recognition}
\end{figure}

The examples are run on a CPU because it was the only common target of
all the tested tools. The performance observed is show in figure
Figure~\ref{fig:results-digit-recognition}.

The details of Figure~\ref{fig:results-digit-recognition} are as follows,
the first two groups of bar graphs represent the observed execution times
with triSYCL using OpenMP for parallelism and triSYCL in OpenCL interoperability
mode. The first two darker bars for each triSYCL mode represent the execution
time when compiled with GCC while the last two represent the execution time when
compiled with clang. The difference between the bars of the same color is that
the left bar represents the execution time with asynchronous kernel execution
enabled while it is disabled in the time shown with the second bar (see
Section~\ref{sec:profiling} for details).
Finally the last two purple and red bars represent the execution time observed
with ComputeCPP and with a ``pure'' OpenCL implementation.


The version written in pure single-source SYCL C++ was run with
triSYCL based on OpenMP and with the free community edition of
ComputeCpp \cite{ComputeCpp}, the major SYCL implementation
\cite{ComputeCpp} targeting OpenCL with SPIR. The version with the
kernel written in OpenCL C directly is run either with the OpenCL host
API directly or with triSYCL and OpenCL interoperability mode.

triSYCL in OpenCL interoperability mode and the pure OpenCL version run
the same OpenCL kernel while triSYCL with OpenMP and ComputeCpp run the
same SYCL kernel.

The data sets can be found on
\url{https://github.com/a-doumoulakis/triSYCL_knn} for further
inspection.

The main part of the SYCL with OpenCL interoperability version is
shown on Figure~\ref{fig:digit-SYCL-OpenCL-code}. Even if the kernel is
described as OpenCL code with the help of Boost.Compute
\cite{Boost.Compute} for conciseness, the data are still stored in
SYCL buffers and their usages are precised by some SYCL
accessors. Compared to the pure SYCL version found in
\url{https://github.com/a-doumoulakis/triSYCL_knn/blob/master/knn_trisycl.cpp},
we built a SYCL kernel from the OpenCL kernel and inside
\lstinline|q.submit()| the \lstinline|cgh.set_args()| connects the
SYCL world to the OpenCL kernel world, connecting the SYCL accessors
to some OpenCL \lstinline|cl_mem| provided to the OpenCL kernel during
execution. The OpenCL kernel is still scheduled asynchronously by the
SYCL runtime according to the task graph as would be a normal SYCL
kernel.

Even if there is no explicit OpenCL buffer motion between host and
device, the SYCL run-time uses the dependency graph constructed by the
accessors to move or not to move the data for kernel execution.

All the measurements were made on the same Linux machine (i7-6700
with 32~GB of RAM) with Ubuntu 16.10, GCC 6.2, Clang 3.8.1
The OpenCL runtime running on the intel CPU was the intel-opencl-runtime 16.1.1
The compiler options can be seen at the GitHub project page.

ComputeCpp on this example is a little bit less performing than the
pure OpenCL solution but does quite well if considered the
productivity gain and that the tool is still a beta version 0.1.3.

triSYCL with OpenMP is not yet optimized on GCC since the project was more
focused up-to-now on experimenting with the specification rather than
towards the performance. But we plan to use the SIMD mode of OpenMP 4
to increase the performances. The Clang implementation of OpenMP already
uses SIMD when running the kernels making the performance similar to OpenCL.


\subsection{Performance profiling of triSYCL}
\label{sec:profiling}
As we have seen with the performance comparison on the knn example,
further improvement can be made to triSYCL to get better performance
both with the OpenCL interoperability mode and with OpenMP parallelism.

In the knn example, we execute a great number of small kernels
(one kernel for each image in the reference set, 500 kernels in total,
with each execution of a kernel taking less than 1ms),
this has highlighted a performance inefficiency in triSYCL.

The current way to add a task in the asynchronous task graph in triSYCL,
is for the main thread to create a new thread for each task,
and then detach it. The new thread will launch the kernel and synchronize by
its own means, so there is no need for the main thread to monitor
its execution. The problem comes when, just like in our example,
the number of tasks is great and the execution time of each task is small.
We can come across situations when we spend more time creating threads
than actually executing them. When executing the kernels with asynchronous 
kernels disabled, the number of threads created decreases dramatically, 
hence the performance improvement observed 
in Figure~\ref{fig:digit-SYCL-OpenCL-code}. 

We ran the example with Intel's OpenCL implementation on CPU that uses TBB
as a backed. With Intel VTune can we profile the execution and we observe
that Intel's OpenCL runtime creates 7 TBB worker threads that will execute
the kernels, meanwhile the main triSYCL thread will, for each kernel execution,
create a thread that will launch the computation of the kernel on the
TBB worker threads. We end up creating more than 500 threads during the
execution of the example with most of these threads delegating the computation
to the TBB workers and doing very little work themselves.

This issue is highlighted even more when using OpenMP. Instead of just using
a pool of existing threads to make the computation, OpenMP creates 8 threads
to execute the workgroups for every execution of a kernel. Again, similarly to
the OpenCL case we create many threads that have a very short lifespan,
for the execution of 500 kernels, 4000 threads are created.

Furthermore, GCC and Clang do not use the same OpenMP library (libgomp for GCC 
and libomp for Clang), Clang reuses existing threads instead of creating new 
ones, this brings the time spent creating threads down and thus an increase
in performance.

The solution to this problem is straight forward, there need to be a way to
reuse threads instead of creating them when possible. This can be achieved
by using a thread pool or a similar mechanism that allows us to assign a task
to an existing thread and monitor the tasks with futures and promises.
Doing that will greatly decrease the amount of time spent creating and
synchronizing the threads between them.


\subsection{Binary Neural Network for image classification on FPGA}
\label{sec:example-from-ken}

\begin{table}
  \caption{Performance results (ms) of the BNN application for 2 host
    API.}
  \begin{tabular}{|c|c|c|c|c|c|}
    \cline{2-6}
    \multicolumn{1}{c|}{}&   \textit{Setup}      & \textit{Memory} & \textit{Build}  & \textit{Kernel} & \textit{Total} \\\hline
    OpenCL& 6490.22     &     .08& 4598.1   & 4.011 &11092.43 \\\hline
    SYCL  & 6411.32     & .002   & 4903.65  & 3.91  &11325.57 \\\hline
  \end{tabular}
  \label{tbl:perfbnn}
\end{table}

\begin{figure*}
  \begin{tabular}{c}
    \begin{lstlisting}[basicstyle=\scriptsize]
[...]
  buffer<long> inputBuffer { inputVector, numInputElems };
  {
    buffer<long> outputBuffer { outputVector, numOutputElems };
    queue.submit([&] (handler &cgh) {
      cgh.setArgs(inputBuffer.get_access<access::mode::read>(cgh),
                  outputBuffer.get_access<access::mode::write>(cgh),
                  false, 0, 0, 0, 0L, count);
      cgh.single_task(kernel);
    });
  }
[...]
\end{lstlisting}
\end{tabular}
\caption{BNN kernel launch using SYCL host API with OpenCL interoperability.}
\label{fig:syclbnn}
\end{figure*}

\begin{figure*}
  \begin{tabular}{c}
    \begin{lstlisting}[basicstyle=\scriptsize]
[...]
  queue.enqueueWriteBuffer(inputBuffer, CL_TRUE, 0, numInputElems, inputVector, NULL, &event);
  kernel.setArgs(0, inputBuffer);
  kernel.setArgs(1, outputBuffer);
  kernel.setArgs(2, false);
  kernel.setArgs(3, 0);
  kernel.setArgs(4, 0);
  kernel.setArgs(5, 0);
  kernel.setArgs(6, 0L);
  kernel.setArgs(7, count);
  queue.enqueueTask(kernel, NULL, &event);
  queue.enqueueReadBuffer(outputBuffer, CL_TRUE, 0, numOutputElems, outputVector, NULL, &event);
[...]
\end{lstlisting}
\end{tabular}
\caption{BNN kernel launch using OpenCL host API.}
\label{fig:openclbnn}
\end{figure*}


In our second example, we present the performance results of a binary
neural network based application, which can accelerate its computation
on an FPGA using SYCL or OpenCL APIs. The SYCL implementation uses
triSYCL with OpenCL interoperabilty mode or by using directly the Xilinx OpenCL implementation, SDAccel. In this particular application, image data is passed as an input to the accelerator for classification. The output is a label of what the image contains.

The kernel code of the application is written in High Level Synthesis
C++, which, when synthesized for the FPGA, acts as a built-in OpenCL
binary kernel that is loaded by either SYCL or OpenCL runtimes.

In Figures~\ref{fig:syclbnn} and \ref{fig:openclbnn}, we display the differences in kernel execution code section of the OpenCL and SYCL implementations. In the case of the SYCL implementation, when the \textit{inputBuffer} is created, its data is loaded from the host to the device from \textit{inputVector}. The code following this is contained within braces, ensuring the kernel has completed execution and the data from the output has been copied back to the host to the \textit{outputVector} before executing subsequent code. The kernel arguments are also set with a single variadic function call. In the case of the OpenCL implementation, copies between host and device are explicit and kernel arguments must be set individually. 

In Table~\ref{tbl:perfbnn}, we report the performance of four critical sections our application and the total execution time for each. The results are an average of 10 executions. Measurements are taken from the wall clock using a high resolution timer. \textit{Setup} accounts for the time to acquire an OpenCL platform, device, context and queue. \textit{Memory} accounts of the buffer allocation time of the input and output. \textit{Build} describes the time taken to read the binary kernel file from disk, build an OpenCL program, and, from it, instantiate the kernel. For both cases, this includes the reconfiguration of the FPGA. \textit{Kernel} accounts for the execution of the kernel, including the setting of arguments and launch. Finally, \textit{Total} accounts for the entirety of the accelerated section, from setup until the results are returned to the host.


Our findings with this example is that SYCL with OpenCL interoperability mode is competitive with directly using OpenCL, with the exception of a small one-time overhead during the build phase. We are currently working to resolve this issue. Encouragingly, the kernel performance between both are comparable.

The memory allocation time is shown as negligible for SYCL compared to
OpenCL because it is done lazily by the runtime and is moved into the
kernel execution time slot.


\section{Related work}
\label{sec:related-work}

The problems addressed here are really meaningful if we consider all
the frameworks developed by a lot of people using heterogeneous
computing. This also means it is impossible to be exhaustive, even by
looking at the C++ frameworks only.

The natural candidate is the second version of the official C++
wrapper from Khronos, \texttt{cl2.hpp} \cite{cl2.hpp}. It is a
straightforward C++ mapping around the OpenCL C API, at least managing
the lifetime of the OpenCL using RAII mechanism. There is a variadic
C++ API to launch kernels, but it is not very high-level compared
to plain OpenCL. Since it is a basic C++ API, this API is often
used by other higher-level API.

For example Boost.Compute \cite{Boost.Compute} is built on top of the
basic previous one and offers also OpenCL API in a C++ mood.
Boost.Compute is actually 2 different API. A basic layer is like
\texttt{cl2.hpp} but in a more modern C++ and STL mind. This is why it
is used by triSYCL. But there is also a higher level API, with
parallel STL-like algorithms operating on device vectors. But there is
no transparent motion of the data of these device vectors between host
and device.

VexCL \cite{VexCL} is higher-level than Boost.Compute since it can
target both OpenCL or CUDA. It provides parallel STL algorithms with
vectors that can be spread across several devices. There is some
support to generate kernels by using symbolic execution. But there is
still some explicit copying required between device vectors and the host domain.

Google contributed the Acxxel library \cite{Acxxel-4.0} inside the
LLVM compiler runtime \texttt{parallel-libs}. It allows programmers to manage
OpenCL and CUDA devices and to launch kernels on them, while unifying
most of the similar concepts on the host side within a unique
syntax. There is no dependency graph between tasks and thus the
transfers between host and devices are still to be done by the
programmer.


\section{Conclusion}
\label{sec:conclusion}

Heterogeneous computing in embedded and high-performance computing is
here to stay because of physical constraints. This puts the pressure on
the programmers to integrate a full system across the various
accelerators. The SYCL standard C++ DSeL allows a single-source
approach for both host and accelerators parts in type-safe way to
simplify the process while interoperable with the ubiquitous C/C++
world. The SYCL runtime provides an implicit task graph managing
asynchronicity and data transfers across the various memory spaces.

Besides this very general programming model, SYCL provides also
interoperability with the OpenCL world, allowing to launch existing
OpenCL kernels while taking advantage of the SYCL framework. While no
longer a single-source programming model in that case, it still
provides the implicit task graph with buffers and accessors, relieving
the programmer to manage explicitly the buffers and memory transfers
between the host and the devices.

This interoperability mode in the SYCL standard has some value by
itself even if this role was not envisioned in the first place. It
allows to use SYCL as a high-level framework to run existing OpenCL
kernels (even built-in kernels controlling networking devices on
FPGA) using a pure C++ approach without the need of a device compiler.

The open-source triSYCL implementation \cite{triSYCL} we are working
on provides also this interoperability mode, as shown with the
application samples presented in this article and the performance
comparisons with other frameworks.

While the performance is generally not better with SYCL than with
plain OpenCL kernels using the OpenCL host API, SYCL increases the
programmer's productivity with the single-source type-safe programming
model for both host code and kernel code and the task graph model with
implicit data transfers.

When interoperability with existing OpenCL code or OpenCL-compatible
kernels is required, for example with FPGA kernels programmed directly
in RTL or HLS C++, the OpenCL interoperability mode of SYCL provides a
nicer host API, simpler to use than the usual OpenCL ones, without
requiring a specific SYCL device compiler.


\bibliographystyle{ACM-Reference-Format}
\bibliography{biblio}

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% TeX-auto-untabify: t
%%% TeX-PDF-mode: t
%%% ispell-local-dictionary: "american"
%%% End:
